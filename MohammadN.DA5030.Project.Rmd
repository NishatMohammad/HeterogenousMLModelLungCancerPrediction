---
title: " DA 5030 Final Project: Lung Cancer Prediction through Machine learning Algorithms"
author: "Dr. Nishat Fatima Mohammad"
date: 04/03/2024
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
  pdf_document:
    latex_engine: xelatex
---

## Business Understanding.  

Lung cancer is the 2nd most common cancer worldwide and in women and the most common cancer in men according to the World Cancer Research Fund International4, with smoking being the most common predisposing factor. Cases have emerged with no decisive hint at the etiology, so we turn to the genetic causes. The interplay of predisposing factors and genes has uplifted the need for personalized medicine and the heterogeneity of cancer is nice and random to apply Machine Learning Algorithms to personalize treatment modalities. Genetic classification of cancers is rapidly becoming the norm and early detection is evermore paramount to screen the patients and their close relatives.  

## Aim of the Ananlysis.  

To leverage appropriate Machine learning algorithms and data mining methods to overcome the complexities of Squamous Cell Lung Cancer RNA-Seq data to predict tumor cases.  

## Data Source.  
The dataset is called TCGAâ€“LUSC (Lung Squamous Cell Carcinoma) Expression Profiling by Array (Microarray) Data set, obtained from Kaggle. It comprises of RNA Seq data from 49 normal cases and 502 squamous cell lung cancer cases. I accessed thecsv.zip file will be working on it in the data exploration section. 

## Load Packages.  
These packages will be required for this analysis. Others will be loaded on the go as required in various code chunks.  

```{r load_packages, message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(tidyr)
library(tibble)
library(gplots)
library(ggplot2)
library(stats)
library(purrr)
library(caret)
library(e1071)
library(randomForest)

```


## 1. Data Exploration:
I will examine the data on a highly level after loading it and wrok forwards thereafter.  

### 1.1. Load and look at the data on a high level
```{r load_data}
# file name
#file <- "LUSCexpfile.csv.zip"
#unzip(file)
unzipd_file <- "LUSCexpfile.csv"

# Load using read.csv
lusc_data <- fread(unzipd_file, sep = ";")

# Explore the data
#glimpse(lusc_data)
#str(lusc_data)
#head(lusc_data)
dim(lusc_data)

```
Here the structure of the data has been looked at after loading the data using `fread()`. This function is faster than `read.csv()` and thus the choice here since we have a 231.2 MB data file.  The `glimpse()` function also gives us a look at the data and the first few rows of the data are looked at using `head()`. It is observed that the columns do not have names. also the genes should be the columns and the patient ids should be the rows. The dimensions are shown above.  

### 1.2. Transpose data.  
The dimensions will be switched here.  

```{r transpose_data}
# Transpose the data frame
lusc_data_t<- as.data.frame(t(lusc_data))

# Gene names to columns
colnames(lusc_data_t) <- lusc_data_t[1, ]
lusc_data_t <- lusc_data_t[-1, ]
#str(lusc_data_t)
#head(lusc_data_t)
dim(lusc_data_t)

```

The tranposed dat now has gene names as the column names with that row deleted. I have commented head and str functions to avoid overwhelming the document with long tables.  

### 1.3. Fix the row and column names and orientation.  
Here, I will try to rename the columns and then switch the rows to columns so we can have the observations in the rows and the variables (genes) in the columns.  

```{r colnames}
# Change row names to id
rownames(lusc_data_t) <- NULL
#head(lusc_data_t)

# Name the first column  outcome
colnames(lusc_data_t)[1] <- "patient_id"
#head(lusc_data_t)
colnames(lusc_data_t)[2] <- "outcome"
outcomes <- unique(lusc_data_t$outcome)

# Take off the pateint id for now
lusc_data_t2 <- lusc_data_t[,2:ncol(lusc_data_t)]
#head(lusc_data_t2)

```

The column with the normal and tumor classes is called outcome and the ID column is called patient_id and taken off.  

### 1.4. Missing values.  
The missing values in this data will be NA, zeros.  

```{r missing_values}
sum(is.na(lusc_data_t2))
# Get the number of transcripts with zero expression
num_zero_genes <- sum(apply(lusc_data_t2, 2, function(row) sum(row == 0)))
num_rows_with_zeros <- sum(apply(lusc_data_t2, 1, function(row) any(row == 0)))
dim(lusc_data_t2)

```

Here there are no missing values in the data. There are zero expression values for variables in every row of the of the data. Recall these variables are trancripts and the values of zero may indicate that the transcript is not expressed in that that observation. I will take off columns with all zero values.  

#### 1.4.1. Handle zero values as Missing values.  
It is impossible to have genes with equal expression all through our samples, thus columns with all zero values and all constant values will be considered as Missing values and will be taken off.  

```{r remove_bad_columns}
# Constant columns
constant_cols <- apply(lusc_data_t2, 2, function(x) length(unique(x)) == 1)
# Remove constant columns
lusc_data_t2 <- lusc_data_t2[, !constant_cols]
dim(lusc_data_t2)

```
The data now has 55,261 columns instead of 56909 columns.  

### 1.5. Visualize data trends.  
Let us take a look at the data in general through visualizations.  

```{r outcome_visuals}
#Get distribution of outcome
outcome_prop <- table(lusc_data_t2$outcome) / length(lusc_data_t2$outcome)
#outcome_prop

# Visualize outcome classes
ggplot(lusc_data_t2, aes(x = outcome, fill = outcome)) +
  geom_bar(color = "black") +
  labs(title = "Distribution of Outcome Variable", x = "Outcome", y = "Frequency") +
  scale_fill_manual(values = c("normal" = "green", "tumor" = "pink"))

```

### 1.6. Encode Features.  

```{r feature_encoding}
# Binary encode the outcome with positive class being tumor
lusc_data_t2$outcome<-as.factor(ifelse(lusc_data_t2$outcome == "tumor", 1, 0))
#head(lusc_data_t2)
#tail(lusc_data_t2)
```

Here the outcome variable is binary encoded 1 for tumor and 0 for normal.  

### 1.7. Feature transformation.  

```{r transform_features}
# Transform all character values to numeric
lusc_data_t2 <- mutate_all(lusc_data_t2, function(x) as.numeric(as.character(x)))
#str(lusc_data_t2)
#head(lusc_data_t2)
any(is.na(lusc_data_t2))

```

All variables are changed to numeric for further analysis, this will be adjusted as per requirements of the models that will be used.  

### 1.8. Visualize with PCA.  
```{r PCA over view, warning=FALSE, message=FALSE}
set.seed(786)
# Perform PCA
pca_result <- prcomp(lusc_data_t2, scale. = TRUE)
#summary(pca_result)
biplot(pca_result)
# Get eigen values
eg_vals <- pca_result$sdev^2
# Get variance explained by each principal component
variance_exp<- eg_vals / sum(eg_vals)
# Visualize
plot(1:length(eg_vals), variance_exp, type = "b", col = "blue", 
     xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained")

```

Here the target variable is the outcome variable in the first column and its distribution can be seen above. The proportionality between the normal and tumor cases is seen in the tumor and visualized with a bar plot. The data is imbalalnced and can pose a problem when training models, in that it will cause the model to be biased towards the highly represented data in this case the tumor cases.  I look at the PCA for an overall look at the data clusters with a scree plot showing the first 6-10 components should be considered for this.  

### 1.9. Data Balancing.  
Here I will demonstrate data balancing on a subset of the data by random oversampling. I have tried to use SMOTE, ROSE without good results and thus relying on rwo() function form the imbalance package. It is arguable that I am balancing the data before splitting which could cause overfitting, but I will do  hyperparametric model tuning and k-fold cross validation to reduce the overfitting that may have been introduced. It is subjective to choose the chronology of steps between balancing, dimensionality reduction and splitting the data. while I must reduce dimensions of my data using PCA before splitting, I must also decide when to do balancing, both cannot be done after splitting, so I choose to balance and reduce dimensionality before splitting and then manage my models to reduce overfitting based on the given time I have to carry out this analysis.  

```{r test_oversampling_on_subset}
set.seed(786)
sub_lusc_dt_normal <- lusc_data_t2[1:10,]
sub_lusc_dt_tumor <- lusc_data_t2[50:70,]
sub_lusc_dt <- data.frame(rbind(sub_lusc_dt_normal, sub_lusc_dt_tumor))
dim(sub_lusc_dt)
sub_outcomes <- sub_lusc_dt[, 1] 
sub_features <- sub_lusc_dt[, -1]

library(imbalance)
extra_normal_outcomes <- rwo(sub_lusc_dt, numInstances =20 , classAttr = "outcome")
dim(extra_normal_outcomes)

balanced_sub_lusc_dt <- rbind(extra_normal_outcomes, sub_lusc_dt)
#dim(balanced_sub_lusc_dt)
any(is.na(balanced_sub_lusc_dt))
zero_cols <- colSums(balanced_sub_lusc_dt == 0) == nrow(balanced_sub_lusc_dt)
#Remove all unexpressed genes (columns with all zero values)
cln_sub_dt <- balanced_sub_lusc_dt[, colSums(balanced_sub_lusc_dt != 0) > 0]
dim(balanced_sub_lusc_dt)
dim(cln_sub_dt)

```

Here, I balanced the data based on the outcome variable to get close to equal number of normal and tumor cases in the subset from the main data as a demo. Now I will try to balance the whole data.  

#### 1.9.1. Balance the whole data.  

```{r balance_complete_data, warning=FALSE, message=FALSE}
# Get outcome and features
all_outcomes <- lusc_data_t2[, 1] 
all_features <- lusc_data_t2[, -1]

#Get the new normal data cases
nw_extra_normal_outcomes <- rwo(lusc_data_t2, numInstances = 500, classAttr = "outcome")
dim(nw_extra_normal_outcomes)
#head(nw_extra_normal_outcomes)
dim(lusc_data_t2)

nw_vars <- intersect(colnames(nw_extra_normal_outcomes), colnames(lusc_data_t2))
complete_balanced_dt <- lusc_data_t2[,nw_vars]
#head(complete_balanced_dt)
nw_extra_normal_outcomes <- nw_extra_normal_outcomes[,nw_vars]
complete_balanced_dt <- rbind(nw_extra_normal_outcomes, complete_balanced_dt)
#head(complete_balanced_dt)


dim(complete_balanced_dt)
any(is.na(complete_balanced_dt))
zero_cols <- colSums(complete_balanced_dt == 0) == nrow(complete_balanced_dt)
#Remove all un expressed genes (columns with all zero values)
cln_balanced_dt <- complete_balanced_dt[, colSums(complete_balanced_dt != 0) > 0]
dim(complete_balanced_dt)
dim(cln_balanced_dt)

#Visualize outcome classes in balanced data
ggplot(cln_balanced_dt, aes(x = outcome, fill = outcome)) +
  geom_bar(color = "red") +
  labs(title = "Distribution of Outcome Variable after balancing the data", x = "Outcome", y = "Frequency")
  
```

Here I successfully balanced the data by random oversampling.  Notice there were a few mistaching columns in the over sampled data and the main data, so I focused on the matching variables and also looked out for variables with all zero values but none were found in the cleaning process evidenced by the same dimensions before and after cleaning. I will be cautious of over fitting on the normal outcome types that may occur due to the use of random oversampling technique. The classes in outcome variable can be seen in the bar plot above.  


## 2. Dimentionality Reduction.  
Considering that I have over 55K features, I have decided to use PCA to carry out dimensionality reduction and will pick the best Principal components for modeling. I have run PCA earlier, and will discuss briefly on how the data clean up has affected the PCA.  

### 2.1 Principal Component Analysis.  

```{r pca2}
set.seed(786)
# Perform PCA
pca_result2 <- prcomp(cln_balanced_dt[, -1], scale. = TRUE)
#pca_result2$sdev
length(pca_result2$sdev)
#pca_result2$rotation
dim(pca_result2$rotation)
#pca_result2$x
dim(pca_result2$x)


# Scree plot
scree_data <- pca_result2$sdev^2
scree_proportion <- scree_data / sum(scree_data)
plot(1:length(scree_proportion), scree_proportion, type = "b", 
     xlab = "Principal Component", ylab = "Proportion of Variance Explained",
     main = "Scree Plot")

```

From the scree plot, I will take the first 30 principal components since they account for around 95% of the variance. Comparing with the previous pca I did this looks more logical considering that data cleaning has been done here to a higher extent than at the point of the earlier PCA.     

### 2.2. Apply PCA for Dimensionality reduction.  

```{r apply_pca2}
#Number of PC to use
top_pc <- 30 
pca_features <- pca_result2$x[, 1:top_pc]
dim(pca_features)
#str(pca_features)
# Get a data frame
pca_features_df <- as.data.frame(pca_features)
colnames(pca_features_df) <- paste0("PC", 1:30)

# Combine with outcome variable
pca_dt <- cbind(cln_balanced_dt$outcome, pca_features_df)
colnames(pca_dt)[1] <- "outcome"
dim(pca_dt)
str(pca_dt)
#head(pca_dt)
#tail(pca_dt)

```

The data now has 30 top principal components as the features in the columns and this will be worked with to split the data.  the new data can be seen above.  

### 2.3. Determine correlation between principal components.  
I have decided to carry out the correlation analysis after the dimensionality reduction considering the number of variables in the original data set were 55000+ that would generate a huge correlation matrix and I expect lots of colinearity as well. itis better then to focus on the mostimportant features after dimentionslity reduction and view the correlation at this point.  

```{r pca_correlation, message=FALSE, warning=FALSE}
set.seed(786)
library(psych)
pairs.panels(pca_dt[,1:15])
pairs.panels(pca_dt[,16:31])

# Spearman correlation between the outcome and PC
cor(pca_dt$outcome,pca_dt[, -1], method = "spearman")
# looking for multicolinearity among pc
head(cor(pca_dt, method="spearman"))

```
The correlation has been visualized in the pairs panel and the correlation of all principal components to the outcome has been looked at through Spearman method (Spearman correlation is robust to outliers, can capture non-linear relationships, popular for biological data).  
PC3,6, 7,9, 10, 23, 24, 27, 28 and 29 have negative correlation coefficients and the rest ar positively correlated to the outcome.  
PC2 is the most strongly and directly correlated to the outcome followed by PC1.  

PC1 and PC2 have a correlation coefficient of about 0.76, showing they capture similar patterns where as PC3 is close to zero showing it captures a separate type of pattern.  


## 3. Data Splitting and Data Stratification.  
I will split the data into 3 parts; training, validation and test sets using 70:15:15 ratio respectively based on the outcome data.  

### 3.1. Training set
```{r Training_set}
set.seed(786)
# Separate the outcome types
outcome1tumor <- pca_dt[pca_dt$outcome == 1, ]
outcome0normal <- pca_dt[pca_dt$outcome == 0, ]
#head(outcome0normal)

# Get nrows for training set
out_neg <- sample(rownames(outcome0normal), 0.70 * nrow(outcome0normal))
out_pos <- sample(rownames(outcome1tumor), 0.70 * nrow(outcome1tumor))

# Get the full training set
training_set <- rbind(pca_dt[out_neg,],pca_dt[out_pos,])
dim(training_set)
#head(training_set)
unique(training_set$outcome)

```
Here the training set has been created with 70% of each class in the outcome variable. Now I will create the validation and sets from the remaining 30 % of the data.  

### 3.2. Validation set.  

```{r Validation_set}
set.seed(786)
# remaining data 
rem_dt <- pca_dt[!(row.names(pca_dt) %in% c(out_neg,out_pos)),]
dim(rem_dt)

# Get the outcome classes
rem_outcome0 <- rem_dt[rem_dt$outcome == 0, ]
rem_outcome1 <- rem_dt[rem_dt$outcome == 1, ]

# Get 50% of each class
rem_out_neg <- sample(rownames(rem_outcome0), 0.50 * nrow(rem_outcome0))
rem_out_pos <- sample(rownames(rem_outcome1), 0.50 * nrow(rem_outcome1))
length(rem_out_pos)

# Get the validation set
validation_set <- rbind(rem_dt[rem_out_neg,],rem_dt[rem_out_pos,])
dim(validation_set)

```
Here the validation set is 15% of the entire data set. I will now create the test set form the final remaining 15%.  


### 3.3. Testing set.  

```{r testing_set}
set.seed(786)
# Get the remaining data for testing set
testing_set <- rem_dt[!(row.names(rem_dt) %in% c(rem_out_neg,rem_out_pos)),]
dim(testing_set)

```

Now the data has been stratified and split into the various sets. I will scale the split sets now.  

## 4. Data transformation and scaling.  

This data is already normalized by TPM Transcripts per Million which is more widely accepted that RPKM and FPKM methods of normalization but scaling the data is still necessary. I will scale the training, validation and test sets separately to avoid data leakage. I will utilize min-max scaling and I will be careful not to scale the outcome variable.  

```{r data_scaling, warning=FALSE}
# Scale training set
trainin_process <- preProcess(as.data.frame(training_set[,-1]), method=c("range"))
traning_set_scald <- predict(trainin_process, as.data.frame(training_set[,-1]))
traning_set_scald <- cbind(training_set[,1], traning_set_scald)
dim(traning_set_scald)
head(traning_set_scald)
#head(training_set)
colnames(traning_set_scald)[1] <- "outcome"
unique(traning_set_scald$outcome)
any(is.na(traning_set_scald))
str(traning_set_scald)

# Scale validation set
validation_process <- preProcess(as.data.frame(validation_set[,-1]), method=c("range"))
validation_set_scald <- predict(validation_process, as.data.frame(validation_set[,-1]))
validation_set_scald <- cbind(validation_set[,1], validation_set_scald)
dim(validation_set_scald)
colnames(validation_set_scald)[1] <- "outcome"
head(validation_set_scald)
unique(validation_set_scald$outcome)
#head(validation_set)

# Scale test set
test_process <- preProcess(as.data.frame(testing_set[,-1]), method=c("range"))
test_set_scald <- predict(test_process, as.data.frame(testing_set[,-1]))
test_set_scald <- cbind(testing_set[,1], test_set_scald)
dim(test_set_scald)
colnames(test_set_scald)[1] <- "outcome"
head(test_set_scald)
#head(testing_set)

```
The split sets have been scaled and the data can be seen in comparison to that before scaling above.  

## 5. Random Forest Model.  

I chose Random forest because I have a whole load of variables which are each very diverse. This allows me to find the important features from the barrage of features upon which I can improve the model.  Random forest models handle overfitting beacuse they are ensemble in nature with random feature selection and sample selection during training that helps to reduce overfitting by creating diverse trees that generalize in a better way on unseen data.

### 5.1. Fit Random forest model and hyperparametric tuning forest parameters.  

```{r fit_rf}
set.seed(786)
# Fit on scaled training set
traning_set_scald$outcome<- as.factor(ifelse(traning_set_scald$outcome == 1, 1, 0))
str(traning_set_scald$outcome)
rf_model1 <- randomForest(outcome ~ ., data = traning_set_scald, ntree = 10,
                          importance = TRUE, proximity = TRUE, type = "classification")
rf_model1

# increase the trees to find best model
rf_model2 <- randomForest(outcome ~ ., data = traning_set_scald, ntree = 15,
                          importance = TRUE, proximity = TRUE, type = "classification")
rf_model2

rf_model3 <- randomForest(outcome ~ ., data = traning_set_scald, ntree = 20,
                          importance = TRUE, proximity = TRUE, type = "classification")
rf_model3

rf_model4 <-randomForest(outcome ~ ., data = traning_set_scald, ntree = 30,
                          importance = TRUE, proximity = TRUE, type = "classification")
rf_model4

rf_model5 <-randomForest(outcome ~ ., data = traning_set_scald, ntree = 40,
                          importance = TRUE, proximity = TRUE, type = "classification")
rf_model5

# Get feature importance and visualize for rf_model4
importance <- importance(rf_model3)
head(importance)
varImpPlot(rf_model3)

```

Models are trained and hyper parametric tuning is carried out by adjusting the number of trees via the `ntree` option. I will move on to evaluate these models.  

### 5.2. Evaluate Random forest models and get accuracy parameters.  

```{r rf_eval}
set.seed(786)
validation_set_scald$outcome<- as.factor(ifelse(validation_set_scald$outcome == 1, 1, 0))
sum(validation_set_scald$outcome == 0)

# Evaluate rf_model1
rf_pred1 <- predict(rf_model1, newdata=validation_set_scald, type="class")
# Get accuracy parameters rf_model1
confusionMatrix(data = rf_pred1, reference = validation_set_scald$outcome, positive = "1")


# Evaluate rf_model2
rf_pred2 <- predict(rf_model2, newdata=validation_set_scald, type="class")
# Get accuracy parameters rf_model2
confusionMatrix(data = rf_pred2, reference = validation_set_scald$outcome, positive = "1")


# Evaluate rf_model3
rf_pred3 <- predict(rf_model3, newdata=validation_set_scald, type="class")
# Get accuracy parameters rf_model3
confusionMatrix(data = rf_pred3, reference = validation_set_scald$outcome, positive = "1")

# Evaluate rf_model4
rf_pred4 <- predict(rf_model4, newdata=validation_set_scald, type="class")
# Get accuracy parameters rf_model4
confusionMatrix(data = rf_pred4, reference = validation_set_scald$outcome, positive = "1")

# Evaluate rf_model5
rf_pred5 <- predict(rf_model5, newdata=validation_set_scald, type="class")
# Get accuracy parameters rf_model5
confusionMatrix(data = rf_pred5, reference = validation_set_scald$outcome, positive = "1")

```
Models created and improved by hyperparametric tuning have been evaluated above.  
From the evaluation the best model is rf_model3 with the best accuracy for tumor class prediction of 75.80%.  


### 5.3. K-Fold cross validation for Random forest model.  

```{r kfold_rf}
set.seed(786)
# Repeated cross validation 
repeat_cv <- trainControl(method='repeatedcv', number=5, repeats=3)

# Train rf_model6
rf_model6 <- train(outcome~., data=traning_set_scald, method='rf', trControl=repeat_cv,
                   ntree = 10, importance = TRUE, proximity = TRUE, 
                   type = "classification", metric='Accuracy')
rf_model6$finalModel

# Evaluate rf_model6
rf_pred6 <- predict(object=rf_model6, newdata=validation_set_scald)

# Accuracy
accuracy_rfmodel6 <- mean(rf_pred6 == validation_set_scald$outcome)*100
accuracy_rfmodel6
#confusion matrix
confusionMatrix(data = rf_pred6, reference = validation_set_scald$outcome, positive = "1")


# Train rf_model7
rf_model7 <- train(outcome~., data=traning_set_scald, method='rf', trControl=repeat_cv,
                   ntree = 20, importance = TRUE, proximity = TRUE, 
                   type = "classification", metric='Accuracy')
rf_model7$finalModel

# Evaluate rf_model7
rf_pred7 <- predict(object=rf_model7, newdata=validation_set_scald)

## Accuracy
accuracy_rfmodel7 <- mean(rf_pred7 == validation_set_scald$outcome)*100
accuracy_rfmodel7

```
Model improvement using K-fold cross validation gave best accuracy with a model of 10 trees, using 5fold  cross validation repeated 3 times. rf_model6 to reduce OOB error rate to 0.14% with accuracy decreasing to 70.70%.  
Based on this the rf_model3 remains a best random forest model followed by rf_model6.  

We can compare both models with ROC.

### 5.4. ROC for the best Random Forest models.  

```{r ROC_rf}
set.seed(786)
library(pROC)
# AUC rf_model3
rf_model3roc <- roc(as.numeric(rf_pred3), as.numeric(validation_set_scald$outcome))
rf_model3auc <- auc(rf_model3roc)
# AUC rf_model6
rf_model6roc <- roc(as.numeric(rf_pred6), as.numeric(validation_set_scald$outcome))
rf_model6rocauc <- auc(rf_model6roc)

# Visualize ROC 
plot(rf_model3roc, col = "purple", main = "ROC Curve - Random forest model 3 vs model 6")
lines(rf_model6roc, col = "pink")
legend("bottomright", legend = c("rf_model3(hyperprametric tuned)", "rf_model6(kfold cross validated)"), col = c("purple", "pink"), lty = 1)

```
From the ROC, rf_model6 remains proves to be the better among the two being compared. I will take rf_model6 as the final random forest model moving forward.  

### 5.5. Feature importance evaluation through Random forest modeling.  

```{r feature_importance, message=FALSE, warning=FALSE}
set.seed(786)
# Get variable importance in a df
importance_rf_model6 <- importance(rf_model6$finalModel)
head(importance_rf_model6)
varImpPlot(rf_model6$finalModel)
important_vars <- varImp(rf_model6, scale=FALSE)
important_vars
# Visualize
important_vars %>%
  ggplot(aes(x=reorder(variables, importance), y=importance)) + 
  geom_bar(stat='identity', fill= "purple") + coord_flip() + xlab('Variables') +
  labs(title='Random forest variable importance') + theme_minimal() + 
  theme(axis.text = element_text(size = 10), axis.title = element_text(size = 15), 
        plot.title = element_text(size = 20))

```
Here is a nice feature of Random forest, the most important features are PC2.  


## 6. C5.0 model.  
The C5.0 decision tree algorithm is robust and interpretable, handles both numerical and categorical data. it recursively partitions the feature space considering the most informative attributes and follows decision rules, increasing model interpretability. It is prone to overfitting if the trees are allowed to grow too deep. Pruning techniques and parameter tuning control overfitting and the built-in mechanisms like winnowing and boosting improve performance to reduce overfitting by iteratively making adjustements based on misclassified instances. for this analyis I will be using boosting methods. For all these reasons, I have added this to the initially proposed list of algos (Random forest, SVM, ANN). 

```{r}
library(C50)
set.seed(786)
# Create the C5 model
c5model1 <- C5.0(outcome ~ ., data = traning_set_scald)
c5model1
summary(c5model1)

# Evaluate with validation data
pred_c5model1 <- predict(c5model1, validation_set_scald)

# Get accuracy parameters
confusionMatrix(data = pred_c5model1, reference = validation_set_scald$outcome, positive = "1")

# ROC C5 model
c5model1roc <- roc(as.numeric(pred_c5model1), as.numeric(validation_set_scald$outcome))
c5model1auc <- auc(c5model1roc)

plot(rf_model6roc, col = "pink", main = "ROC Curve - Random forest model 3 vs c5model1")
lines(c5model1roc, col = "green")
legend("bottomright", legend = c("rf_model3(hyperprametric tuned)", "c5model1"), col = c("pink", "green"), lty = 1)

```
The c5model1 has been fit on the training data, evaluated with the validation data and accuracy metrics generated as shown above with accuracy of 71.34% which is close to that of th random forest model.
The decision tree consists of only 2 nodes(the model is simple). The split is based on the predictor PC2 with a threshold of 0.6731152. There is an error rate of 0.1% on the training data, with 1 misclassified case out of 735. one predictor (PC2) is used for splitting thus lack of complexity in this model. The confusion matrix shows that the model correctly classified 30 out of 75 of the positive class (tumor) and correctly classified of the negative class (normal). The c5model1 has specificity of 1.0000 while the sensitivity (true positive rate) is relatively low at 0.4000.  
Comparing both based on the ROC, the c5model1 and the random forest model rf_model6 are really close matches to each other.  

### 6.1. Boosting and Kfold cross validation for C5.0 model.  

```{r boost_kfold_cv, message=FALSE, warning=FALSE}
# Cross validation 
c50cv <- trainControl(method = "cv", number = 5)

#tuning params
C50_tune_grid <- expand.grid(.model = c("tree"), .trials = 1:10, .winnow = FALSE)

# Create a boosted C5.0 model
c5model_boost <- train(outcome ~ ., data = traning_set_scald, method = "C5.0", trControl = c50cv, tuneGrid = C50_tune_grid)
c5model_boost

# evaluate boosted model
c5model_pred2 <- predict(c5model_boost, validation_set_scald)

# Confusion matrix
confusionMatrix(data = c5model_pred2, reference = validation_set_scald$outcome, positive = "1")

# ROC C5 model
c5model_boost_roc <- roc(as.numeric(c5model_pred2), as.numeric(validation_set_scald$outcome))
c5model_boost_auc <- auc(c5model_boost_roc)

# Plot ROC curves
plot(rf_model6roc, col = "pink", main = "ROC Curve - Random forest model 3 vs Boosted C5.0 model")
lines(c5model1roc, col = "green")
lines(c5model_boost_roc, col = "darkgreen")
legend("bottomright", legend = c("Random forest model 3", "c5model1 (base model)",  "Boosted C5.0 model"), col = c("pink", "green", "darkgreen"), lty = 1)

```

Here the accuracy parameters remain the same for the base model and the boosted model. I will go with the base model.  


## 7. SVM model.  
Support Vector Machines are good for prediction tasks because they handle high-dimensional data, robust against overfitting and generalize unseen examples through kernel functions, and suitable for linear and non-linear classification problems. This model is less prone to overfitting with the regularization parameter ,C, handling overfitting and getting at a simpler decision boundary that generalizes unseen data. I proposed its use from the start and I have great expectations from this model as well as the previous two.  

```{r svm_model}
set.seed(786)
# Fit SVM model
svm_model1 <- svm(outcome ~ ., data = traning_set_scald, kernel = "linear")
svm_model1

# Model evaluation
svm_pred1 <- predict(svm_model1, newdata = validation_set_scald)

# Get confusion matrix
confusionMatrix(data = svm_pred1, reference = validation_set_scald$outcome,
                positive = "1")

table(Actual = validation_set_scald$outcome, 
                                     Predicted = svm_pred1)

```
For svm_model1, the accuracy is at 96.18% .  

### 7.1. SVM model improvement by regularization of C and use of k-fold crossvalidation.  

```{r SVM_model_improvement}
set.seed(786)
svm_grid1 <- expand.grid(C = 0.1)

# Perform cross-validated grid search with linear SVM
svm_tuned <- train(outcome ~ ., 
                   data = traning_set_scald, 
                   method = "svmLinear",
                   trControl = trainControl(method = "cv", number = 10),
                   tuneGrid = svm_grid1)

svm_grid2 <- expand.grid(C = c(0.1, 1, 10))
svm_tuned2 <- train(outcome ~ ., 
                   data = traning_set_scald, 
                   method = "svmLinear",
                   trControl = trainControl(method = "cv", number = 10),
                   tuneGrid = svm_grid2)


# Check the best model
svm_model2 <- svm_tuned$finalModel
svm_model2
svm_model3 <- svm_tuned2$finalModel
svm_model3
# Evaluate improved model
svm_pred2 <- predict(svm_tuned, newdata = validation_set_scald)

#Get confusion matrix for improved model
confusionMatrix(data = svm_pred2, reference = validation_set_scald$outcome,
                positive = "1")


# Try radial kernel
library(ROCR)
library(kernlab)
# Cross validation
set.seed(786)
ctrl_radial <- trainControl(method="cv",
                     number = 2,
                     summaryFunction=twoClassSummary,
                     classProbs=TRUE)

#  Fine tune with grid search 
grid_radial <- expand.grid(sigma = c(.01, .015, 0.2), C = c(0.75, 0.9, 1, 1.1, 1.25))

training_set_features <- traning_set_scald[,-1]
training_set_outcome <- traning_set_scald[,1]
str(training_set_outcome)
levels(training_set_outcome)  <- c("normal", "tumor")
#Train SVM
svm_tune_radial <- train(x=training_set_features, y= training_set_outcome,
                  method = "svmRadial", metric="ROC", tuneGrid = grid_radial,
                  trControl=ctrl_radial)

svm_tune_radial

# Predict Target Label
validation_features <-validation_set_scald[,-1]
validation_outcomes <- validation_set_scald[, 1]
str(validation_outcomes)
levels(validation_outcomes) <- c("normal", "tumor")
svm_pred3 <- predict(svm_tune_radial, validation_features, type="prob")[2]
head(svm_pred3)
# Model Performance Statistics
svm_radial_pred_val <-prediction(svm_pred3[,1], validation_outcomes)

# Get AUC
performance_val <- performance(svm_radial_pred_val,"auc")
performance_val

# Get tpr and fpr
performance_val <- performance(svm_radial_pred_val, "tpr", "fpr")
performance_val
# Plot ROC
plot(performance_val, col = "brown", lwd = 1.5)

```

The svm_tuned model has utilized 10 fold stratified cross validation, and tuned on the cost by changing it 0.1. This improved the model slightly with accuracy increasing to 97.45%. this is the best svm model.  

I tried to tune the model to a nonlinear radial kernel with a range of cost and gamma values, but did not get a proper ROC curve and thus I will take the linear svm tuned model as the best among the three svm models.  


## 8. ANN model.  
I will leverage the perculiarities of ANN hidden layers, ability to cope with multi-dimensionality and capability to pick up patterns to model upon to predict the outcome.  To handle overfitting, the ANN model can leverage on L1 and L2 regularization, cross fold validation or drop out techniques. I will use k-fold cross validation to combat overfitting.  

```{r ann_classifier}
library(DescTools)
library(NeuralNetTools)
set.seed(786)
# fit annmodel
annmodel <- train(form = outcome ~.,data = traning_set_scald,
                  method = "nnet",trace = FALSE)
annmodel
plotnet(annmodel, pos_col = "pink", neg_col = "green", bias = FALSE,
        circle_cex = 4, cex_val = 0.8)

# Evaluate annmodel
annmodel_pred <- predict(object = annmodel,newdata = validation_set_scald)
# Confusion matrix annmodel
confusionMatrix(data = annmodel_pred, 
                reference = validation_set_scald$outcome,positive = "1")

```
The first ann model is fitted, evaluated and confusion matrix generated shows an accuracy of 56.05%. I will go ahead to fine tune the model for better performance.  

### 8.1. Model Improvement by hyperparametric tuning decay and size plus kfold cross validation.  

```{r ann_model_imporvement}
set.seed(786)
# cross validation params
ann_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3, search = "grid")

# grid tuning ranges
ann_grid <- expand.grid(size = seq(from = 1, to = 7, by = 2),
                      decay = seq(from = 0, to = 0.1, by = 0.01))

# Fit tuned ann model
set.seed(786)
ann_model_tuned <- train(form = outcome ~., data = traning_set_scald, 
                         method = "nnet", trControl = ann_ctrl,
                         tuneGrid = ann_grid, trace = FALSE)

#Visualize model
plotnet(mod_in = ann_model_tuned$finalModel, pos_col = "pink", neg_col = "green",
        bias = FALSE, circle_cex = 4, cex_val = 0.8) 

# Evaluate ann_model_tuned
annmodel_pred <- predict(object = ann_model_tuned,newdata = validation_set_scald)
# Confusion matrix ann_model_tuned
confusionMatrix(data = ann_model_tuned, 
                reference = validation_set_scald$outcome,positive = "1")

```
The model has been tuned through ranges of values for decay and size. A 5-fold cross validation has been done with 3 repeats. These led to an accuracy of 99.73%. The confusion matrix did not show other accuracy parameters. I am unable to get the ROC curves for the ann models so I will compare them with other models based on F1 statistic and recall. 

## 9. Model comparison.  

### 9.1. Individual model comparison.  

Here I will use basic accuracy parameters to get the Recall and F1 scores and compare models based on these. ROC comparisons have already been made above.  

Accuracy= TP+TN/(TP+TN+FP+FN)


Precision/Positive Predictive Value
= TP/(TP+FP)

Recall/Sensitivity/True Positive Rate 
= TP/(TP+FN)

Specificity/True Negative Rate 
= TN/(TN+FP)

False Positive Rate/Fall-Out/False positive rate 
= 1- specificity OR FP/(TN + FP)

F1 Score 
= 2((precision * Recall)/(Precion + Recall))

NB:
TP=True Positives
TN=True Negatives
FP=False Positives
FN=False Negatives

Based on these, I derived the data below for each model:

|   Parameter         |   rf_model6        |   c5model1     |
| :---:               | :---:              | :---:          |
| Overall Accuracy    | 0.707              | 0.7134         |
| Precision           | 1.000              | 1.0000         |
| Sensitivity/Recall  | 0.3867             | 0.4000         |
| F1-Score            | 0.5578             | 0.5714         |
| AUC for ROC         |`r rf_model6rocauc` | `r c5model1auc`|  


|   Parameter         |   svm_tuned     |   ann_model_tuned   |
| :---:               | :---:           | :---:               |
| Overall Accuracy    | 0.9045          | 0.9973              |
| Precision           | 1.0000          | 0.9926              |
| Sensitivity/Recall  | 0.8000          | 1.0000              |
| F1-Score            | 0.8889          | 0.9939              |
| AUC for ROC         | not obtained    | not obtained        |  

From the above the SVM and ann models outperform the tree models, although I could not obtain the AUC and ROC for these to compare further.  I would pick the SVM model as the best because I consider the ann model may be on the overfitted end  (considering the broad difference between accuracy parameters of the untuned and tuned models).  

## 10. Heterogenous Ensemble function with all models.   
I will create a function to predict using the above models and take the majority vote for the outcome.  

```{r het_ens_function}
set.seed(786)
predictOutcomeClass <- function(data) {
  # Predictions
  rf_pred <- predict(rf_model6, newdata = data, type = "prob")
  rf_pred_class <- ifelse(rf_pred > 0.5, "1", "0")
  c5_pred <- predict(c5model1, newdata = data)
  svm_pred <- predict(svm_tuned, newdata = data)
  ann_pred <- predict(ann_model_tuned, newdata = data)
  all_preds <- data.frame(rf = rf_pred_class, c5 = c5_pred, svm = svm_pred, ann = ann_pred)
  # Get majority vote
  majority <- apply(all_preds, 1, function(x) {
    names(sort(table(x), decreasing = TRUE)[1])
  })
  
  return(majority)
}

# Get predictions for validation data
ensemble_val_pred <- predictOutcomeClass(validation_set_scald)
table(validation_set_scald$outcome, ensemble_val_pred)

# ROC
het_ens_roc1 <- roc(as.numeric(ensemble_val_pred), as.numeric(validation_set_scald$outcome))
het_ens_auc1 <- auc(het_ens_roc1)

plot(het_ens_roc1, col = "violet", main = "ROC Curve - Heterogenous Ensemble model vs others")
lines(c5model1roc, col = "green")
lines(rf_model6roc, col = "pink")
legend("bottomright", legend = c("het_ens_model", "c5model1", "rf_model6(kfold cross validated)"), col = c("violet", "green", "pink"), lty = 1)

```
Here I made the heterogenous ensemble model which calls all the models created and takes a majority vote, evaluated on the validation data with the confusion matrix shown above and the accuracy parameters were calculated based on the formulae shown earlier to get the following for the heterogenous ensemble model:

Accuracy - 0.9045
Precision - 1.000
Recall (Sensitivity) - 0.8000
F1 Score -  0.8889
Specificity - 1.000
AUC - `r het_ens_auc1`

This provides an over all outcome, it is better in performance than the random forest and c5.0 models, it has similar outputs to the SVM model which I mentioned could be potentially the best among all. It does not look overfitted like the tuned ann model. ROC comparison with the tree models show pretty close match between the three.   

The ensemble model is the best model in this analysis. I will now predict the outcome for the testing set which been on hold till the end of the analysis.

## 11. Final prediction and evaluation on the testing data set by Hold out method.  

```{r final_pred}
set.seed(786)
str(test_set_scald$outcome)
length(test_set_scald$outcome)
unique(test_set_scald$outcome)
# Factor outcome in test set 
test_set_scald$outcome <- as.factor(ifelse(test_set_scald$outcome == 1, 1, 0))

# Final pred
final_ens_pred <- predictOutcomeClass(test_set_scald[,-1])

# confusion matrix
table(test_set_scald$outcome, final_ens_pred)


```
The test data shos the following calculated accuracy parameters from the confusion matrix:

Accuracy - 0.89
Precision - 1.0
Recall (Sensitivity) - 0.78
Specificity - 0.830
F1 Score - 0.88

I expected a reduction in performance as this is usually the case when the data integrity is kept for test set. the hold out method on the test data has paid off with great results nonetheless, the accuracy remains close to that acheieved with training and validation sets at 89%, and F1 score or the harmonic mean is also reduced at 88%. This is expected and although I expected to have some degree of over fitting here, the difference on metrics performance between the training, validation and test sets is about 1% and thus there is minimal overfitting occuring in our models, implying the robustness of hyperparametric tuning and  k-fold cross validation strategies for model improvement and evaluation.  

## 12. Conclusion.  
A large part of this analysis dwelt on data clean up, transformation, data balancing, dimensionality reduction and splitting and scaling, with look out for overfitting due the choice of chronology of the steps.  
The models form Random forest, C5.0 decision trees, Support Vector Machines and Artificial Neural Networks have been used in this analysis to address the data complexity and the issue of overfitting by implementing hyper parametric tuning and use of K-fold cross validation. Careful assessments were made using accuracy parameters and best models where called in a heterogenous ensemble model to get the majority vote on predictions of classes of the outcome of the instances. the hold out method of evaluation was very useful at the end of analysis and proved valuable in evaluation of the ensemble model. 
The accuracy of the heterogenous ensemble model is most reliable keeping in mind it has leveraged upon the best outcomes of all the best models in this analysis  with an accuracy of 89.2%, Precision of 77.6%, Recall of 100%, specificity of 83% and F1 Score or harmonic mean of 87.3%.  

This analysis has been an informative and exciting one. Many appreciations to my Instructor Dr Amin Assareh, the Teaching Assistants especially Aayush Naveen Halgekar and Suprajasai Konegari for their patience and efforts through out the course this spring semester.  Many appreciations to my family for patiently waiting in all my absences and so many thanks to the readers of this document for going through it till the end.  

## References:
1. TCGAâ€“LUSC (Lung Squamous Cell Carcinoma) Expression Profiling by Array (Microarray) Data set. Retrieved 2024. https://www.kaggle.com/datasets/noepinefrin/tcga-lusc-lung-cell-squamous-carcinoma-gene-exp/data.  

2. Bioinformatics Approach | 99.1% Accuracy with DEGs. Retrieved 2024. https://www.kaggle.com/code/noepinefrin/bioinformatics-approach-99-1-accuracy-with-degs/notebook.  

3. TCGA_LUSC_PCA_CatBoost_BalancedAccuracy=100%. Retrieved 2024. https://www.kaggle.com/code/bryamblasrimac/tcga-lusc-pca-catboost-balancedaccuracy-100.  

4. World Cancer Research Fund International. Retrieved 2024. https://www.wcrf.org/cancer-trends/lung-cancer-statistics/.  

5. Lantz, B., 2019. Machine Learning with R (4th ed.). Packt Publishing.  

6. Bhalla D., Retrived 2024, Listen Data, Support Vector Machines in R Simplified. https://rpubs.com/chelseyhill/733053.  

7. Hill C. , Retrived 2024, Classification Analysis: Artifical Neural Networks. https://rpubs.com/chelseyhill/733053.  

8. Muralidhar. K. 2021, Medium: Learning Curve to identify Overfitting and Underfitting in Machine Learning. https://towardsdatascience.com/learning-curve-to-identify-overfitting-underfitting-problems-133177f38df5.  

